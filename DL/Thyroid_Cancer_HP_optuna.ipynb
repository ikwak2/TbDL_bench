{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, Module\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchmetrics import AUROC, Accuracy, MeanSquaredError\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_frame import stype\n",
    "from torch_frame.data import Dataset, DataLoader\n",
    "from torch_frame.datasets import (\n",
    "    ForestCoverType, KDDCensusIncome, DataFrameBenchmark,\n",
    "    AdultCensusIncome, BankMarketing, Dota2\n",
    ")\n",
    "from torch_frame.gbdt import CatBoost, LightGBM, XGBoost\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder, FTTransformer, LinearBucketEncoder,\n",
    "    LinearEncoder, LinearPeriodicEncoder, ResNet, TabNet, TabTransformer\n",
    ")\n",
    "from torch_frame.nn.models import (\n",
    "    MLP, ExcelFormer, Trompt\n",
    ")\n",
    "from torch_frame.typing import TaskType\n",
    "\n",
    "\n",
    "\n",
    "# Use GPU for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"thyroid_cancer_risk_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 task\n",
    "is_classification = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_frame import numerical, categorical, text_embedded, embedding\n",
    "\n",
    "## 칼럼 별 Dtype 지정 \n",
    "col_to_stype={#\"Patient_ID\" : numerical,\n",
    "              \"Age\" : numerical,\n",
    "              \"Gender\" : categorical,\n",
    "              \"Country\" : categorical,\n",
    "              \"Ethnicity\" : categorical,\n",
    "              \"Family_History\" : categorical,\n",
    "              \"Radiation_Exposure\" : categorical,\n",
    "              \"Iodine_Deficiency\" : categorical,\n",
    "              \"Smoking\" : categorical,\n",
    "              \"Obesity\" : categorical,\n",
    "              \"Diabetes\" : categorical,\n",
    "              \"TSH_Level\" : numerical,\n",
    "              \"T3_Level\" : numerical,\n",
    "              \"T4_Level\" : numerical,\n",
    "              \"Nodule_Size\" : categorical,\n",
    "              \"Thyroid_Cancer_Risk\" : categorical,\n",
    "              \"Diagnosis\" : categorical}\n",
    "\n",
    "dataset = Dataset(df = df, \n",
    "                  col_to_stype = col_to_stype, \n",
    "                  target_col = \"Diagnosis\")\n",
    "\n",
    "dataset.materialize()\n",
    "\n",
    "## split\n",
    "train_dataset, val_dataset, test_dataset = dataset[:0.6], dataset[0.6:0.7], dataset[0.7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet / FT-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--compile'], dest='compile', nargs=0, const=True, default=False, type=None, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='adult')\n",
    "parser.add_argument('--numerical_encoder_type', type=str, default='linear',\n",
    "                    choices=['linear', 'linearbucket', 'linearperiodic'])\n",
    "parser.add_argument('--model_type', type=str, default='fttransformer',\n",
    "                    choices=['fttransformer', 'resnet'])\n",
    "parser.add_argument('--channels', type=int, default=256)\n",
    "parser.add_argument('--num_layers', type=int, default=4)\n",
    "parser.add_argument('--batch_size', type=int, default=512)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--compile', action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter에서 실행될 때는 sys.argv를 조정\n",
    "args = parser.parse_args([\n",
    "    #'--dataset', 'adult',\n",
    "    #'--numerical_encoder_type', 'linear',\n",
    "    #'--model_type', 'resnet',       # fttransformer : FT-T / resnet : ResNet\n",
    "    #'--channels', '256',\n",
    "    #'--num_layers', '4',\n",
    "    #'--batch_size', '256',  # 데이터를 256개씩 한번에 \n",
    "    #'--lr', '0.0001',\n",
    "    '--epochs', '10',\n",
    "    #'--seed', '0'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: numerical_encoder,\n",
    "}\n",
    "\n",
    "if is_classification:\n",
    "    #output_channels = dataset.num_classes    ->   contains StatType.COUNT을 포함하지 않아서 오류(?)\n",
    "    output_channels = 2 # 그냥 수동으로 설정.,,,,   => 분류 칼럼 unique 개수로 설정 \n",
    "else:\n",
    "    output_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    loss_sum = sample_cnt = 0\n",
    "\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf)\n",
    "\n",
    "        loss = F.cross_entropy(pred, tf.y.long()) if is_classification \\\n",
    "               else F.mse_loss(pred.view(-1), tf.y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum   += loss.item() * len(tf.y)\n",
    "        sample_cnt += len(tf.y)\n",
    "\n",
    "    return loss_sum / sample_cnt\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct_or_error, sample_cnt = 0, 0\n",
    "\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf)\n",
    "\n",
    "        if is_classification:\n",
    "            correct_or_error += (pred.argmax(dim=-1) == tf.y).sum().item()\n",
    "        else:\n",
    "            correct_or_error += F.mse_loss(\n",
    "                pred.view(-1), tf.y.view(-1), reduction='sum'\n",
    "            ).item()\n",
    "        sample_cnt += len(tf.y)\n",
    "\n",
    "    if is_classification:\n",
    "        return correct_or_error / sample_cnt      # accuracy ↑\n",
    "    else:\n",
    "        return (correct_or_error / sample_cnt) ** 0.5   # RMSE ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────── Optuna objective ──────────────────────── #\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # ── 1) 탐색할 하이퍼파라미터 ────────────────────────── #\n",
    "    model_type             = trial.suggest_categorical(\"model_type\", [\"fttransformer\"])    # resnet / fttransformer \n",
    "    numerical_encoder_type = trial.suggest_categorical(\n",
    "        \"numerical_encoder_type\", [\"linear\", \"linearbucket\", \"linearperiodic\"])\n",
    "    channels   = trial.suggest_categorical(\"channels\", [128, 256, 512])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
    "    lr         = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # ── 2) 데이터로더 (배치 크기마다 새로 만듦) ─────────── #\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset.tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(\n",
    "        val_dataset.tensor_frame,   batch_size=batch_size)\n",
    "    \n",
    "    # ── 3) 각 trial-별 인코더 구성 ─────────────────────── #\n",
    "    if numerical_encoder_type == \"linear\":\n",
    "        numerical_encoder = LinearEncoder()\n",
    "    elif numerical_encoder_type == \"linearbucket\":\n",
    "        numerical_encoder = LinearBucketEncoder()\n",
    "    else:\n",
    "        numerical_encoder = LinearPeriodicEncoder()\n",
    "\n",
    "    stype_encoder_dict = {\n",
    "        stype.categorical: EmbeddingEncoder(),\n",
    "        stype.numerical:   numerical_encoder,\n",
    "    }\n",
    "\n",
    "    output_channels = 2  # Diagnosis 클래스 수\n",
    "    # ── 4) 모델 생성 ─────────────────────────────────── #\n",
    "    if model_type == \"fttransformer\":\n",
    "        model = FTTransformer(\n",
    "            channels=channels,\n",
    "            out_channels=output_channels,\n",
    "            num_layers=num_layers,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=train_dataset.tensor_frame.col_names_dict,\n",
    "            stype_encoder_dict=stype_encoder_dict,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = ResNet(\n",
    "            channels=channels,\n",
    "            out_channels=output_channels,\n",
    "            num_layers=num_layers,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=train_dataset.tensor_frame.col_names_dict,\n",
    "        ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # ── 5) 학습 loop (epoch 10 고정) + Pruning 보고 ───── #\n",
    "    for epoch in range(1, 11):  # 1~10\n",
    "        train_one_epoch(model, train_loader, optimizer)\n",
    "        val_score = evaluate(model, val_loader)\n",
    "\n",
    "        # Optuna에 현재 epoch의 score 보고\n",
    "        trial.report(val_score, step=epoch)\n",
    "\n",
    "        # MedianPruner가 “쓸모없다”고 판단하면 중단\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_score     # 방향: 분류면 maximize(accuracy), 회귀면 minimize(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-21 05:59:28,793] A new study created in memory with name: no-name-0794afc2-0c3b-410b-945c-7f105f5edec0\n",
      "[I 2025-05-21 06:02:01,766] Trial 0 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearperiodic', 'channels': 256, 'num_layers': 4, 'batch_size': 128, 'lr': 0.00014885411789079686}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:03:45,475] Trial 1 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearbucket', 'channels': 256, 'num_layers': 5, 'batch_size': 256, 'lr': 3.0031027460910578e-05}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:05:04,514] Trial 2 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linear', 'channels': 128, 'num_layers': 3, 'batch_size': 512, 'lr': 2.583559406337251e-05}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:07:26,528] Trial 3 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearbucket', 'channels': 512, 'num_layers': 4, 'batch_size': 256, 'lr': 7.423292697379833e-05}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:08:49,962] Trial 4 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearbucket', 'channels': 256, 'num_layers': 2, 'batch_size': 512, 'lr': 0.00023244548604818048}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:10:37,950] Trial 5 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearperiodic', 'channels': 512, 'num_layers': 2, 'batch_size': 512, 'lr': 5.113584093845827e-05}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:12:20,146] Trial 6 finished with value: 0.8306925572429358 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearperiodic', 'channels': 512, 'num_layers': 2, 'batch_size': 512, 'lr': 0.00014181279603364065}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:13:48,220] Trial 7 finished with value: 0.830927641167897 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linearperiodic', 'channels': 256, 'num_layers': 3, 'batch_size': 512, 'lr': 1.0855263229755759e-05}. Best is trial 0 with value: 0.830927641167897.\n",
      "[I 2025-05-21 06:15:47,419] Trial 8 finished with value: 0.8305044901029668 and parameters: {'model_type': 'fttransformer', 'numerical_encoder_type': 'linear', 'channels': 256, 'num_layers': 3, 'batch_size': 128, 'lr': 0.00022554914569542715}. Best is trial 0 with value: 0.830927641167897.\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────── Optuna Study 실행 ──────────────────────── #\n",
    "direction = \"maximize\" if is_classification else \"minimize\"\n",
    "pruner    = optuna.pruners.MedianPruner(n_warmup_steps=3)\n",
    "\n",
    "study = optuna.create_study(direction=direction, pruner=pruner)\n",
    "study.optimize(objective, n_trials=50, timeout=60*60)   # 예: 50 trial or 1시간 제한\n",
    "\n",
    "# ──────────────────────── 결과 확인 ──────────────────────── #\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best {direction} value : {best_trial.value:.4f}\")\n",
    "print(\"Best hyper-parameters  :\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k:25s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선택된 파라미터로 다시 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Best hyper-parameters\n",
      "  model_type               : resnet\n",
      "  numerical_encoder_type   : linearperiodic\n",
      "  channels                 : 512\n",
      "  num_layers               : 5\n",
      "  batch_size               : 512\n",
      "  lr                       : 0.00015036039946508727\n",
      "[Fin] Epoch 01 | Train Acc: 0.8276\n",
      "[Fin] Epoch 02 | Train Acc: 0.8276\n",
      "[Fin] Epoch 03 | Train Acc: 0.8276\n",
      "[Fin] Epoch 04 | Train Acc: 0.8276\n",
      "[Fin] Epoch 05 | Train Acc: 0.8276\n",
      "[Fin] Epoch 06 | Train Acc: 0.8276\n",
      "[Fin] Epoch 07 | Train Acc: 0.8276\n",
      "[Fin] Epoch 08 | Train Acc: 0.8276\n",
      "[Fin] Epoch 09 | Train Acc: 0.8276\n",
      "[Fin] Epoch 10 | Train Acc: 0.8276\n",
      "\n",
      "★ Final Accuracy on TEST set: 0.8268\n"
     ]
    }
   ],
   "source": [
    "# ─── 1. 최적 하이퍼파라미터 가져오기 ───────────────────────────── #\n",
    "best_params    = study.best_trial.params\n",
    "model_type             = best_params[\"model_type\"]\n",
    "numerical_encoder_type = best_params[\"numerical_encoder_type\"]\n",
    "channels   = best_params[\"channels\"]\n",
    "num_layers = best_params[\"num_layers\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "lr         = best_params[\"lr\"]\n",
    "\n",
    "print(\"▶ Best hyper-parameters\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k:25s}: {v}\")\n",
    "\n",
    "# ─── 2. 데이터로더 (train+val 통합) ───────────────────────────── #\n",
    "full_train_dataset = dataset[:0.7]           # train 60% + val 10%  → 70%\n",
    "full_train_loader  = DataLoader(\n",
    "    full_train_dataset.tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset.tensor_frame, batch_size=batch_size)\n",
    "\n",
    "# ─── 3. 인코더·모델 구성 ─────────────────────────────────────── #\n",
    "if numerical_encoder_type == \"linear\":\n",
    "    numerical_encoder = LinearEncoder()\n",
    "elif numerical_encoder_type == \"linearbucket\":\n",
    "    numerical_encoder = LinearBucketEncoder()\n",
    "else:\n",
    "    numerical_encoder = LinearPeriodicEncoder()\n",
    "\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical:   numerical_encoder,\n",
    "}\n",
    "\n",
    "output_channels = 2     # Diagnosis 클래스 수\n",
    "if model_type == \"fttransformer\":\n",
    "    final_model = FTTransformer(\n",
    "        channels=channels,\n",
    "        out_channels=output_channels,\n",
    "        num_layers=num_layers,\n",
    "        col_stats=dataset.col_stats,\n",
    "        col_names_dict=full_train_dataset.tensor_frame.col_names_dict,\n",
    "        stype_encoder_dict=stype_encoder_dict,\n",
    "    ).to(device)\n",
    "else:\n",
    "    final_model = ResNet(\n",
    "        channels=channels,\n",
    "        out_channels=output_channels,\n",
    "        num_layers=num_layers,\n",
    "        col_stats=dataset.col_stats,\n",
    "        col_names_dict=full_train_dataset.tensor_frame.col_names_dict,\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=lr)\n",
    "\n",
    "# ─── 4. 학습(Epoch 10 고정) ─────────────────────────────────── #\n",
    "for epoch in range(1, 11):\n",
    "    train_one_epoch(final_model, full_train_loader, optimizer)\n",
    "    train_acc = evaluate(final_model, full_train_loader)\n",
    "    print(f\"[Fin] Epoch {epoch:02d} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "# ─── 5. 최종 테스트 ─────────────────────────────────────────── #\n",
    "test_metric = evaluate(final_model, test_loader)\n",
    "metric_name = \"Accuracy\" if is_classification else \"RMSE\"\n",
    "print(f\"\\n★ Final {metric_name} on TEST set: {test_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "is_classification = True\n",
    "\n",
    "# ─────────────────── 데이터 로더 생성 함수 ──────────────────── #\n",
    "def make_loaders(batch_size, use_val=True):\n",
    "    train_tf = train_dataset.tensor_frame\n",
    "    val_tf   = val_dataset.tensor_frame\n",
    "    test_tf  = test_dataset.tensor_frame\n",
    "    train_loader = DataLoader(train_tf, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_tf,   batch_size=batch_size) if use_val else None\n",
    "    test_loader  = DataLoader(test_tf,  batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# ─────────────────── 공통 train / evaluate ──────────────────── #\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    loss_sum = samp = 0\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf)\n",
    "        loss = F.cross_entropy(pred, tf.y.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * len(tf.y)\n",
    "        samp     += len(tf.y)\n",
    "    return loss_sum / samp\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf).argmax(dim=-1)\n",
    "        correct += (pred == tf.y).sum().item()\n",
    "        total   += len(tf.y)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# ─────────────────── Optuna objective ──────────────────── #\n",
    "def objective(trial):\n",
    "    # 1) 하이퍼파라미터 샘플링\n",
    "    params = dict(\n",
    "        channels   = trial.suggest_categorical(\"channels\",  [128, 256, 512]),\n",
    "        gamma      = trial.suggest_float(\"gamma\",    1.0, 2.0),\n",
    "        num_layers = trial.suggest_int (\"num_layers\", 2, 6),\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512]),\n",
    "        lr         = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "    )\n",
    "\n",
    "    # 2) 데이터로더\n",
    "    train_loader, val_loader, _ = make_loaders(params[\"batch_size\"], use_val=True)\n",
    "\n",
    "    # 3) 모델\n",
    "    model = TabNet(\n",
    "        out_channels = 2,                                   # 클래스 수\n",
    "        num_layers   = params[\"num_layers\"],\n",
    "        split_attn_channels  = params[\"channels\"],\n",
    "        split_feat_channels  = params[\"channels\"],\n",
    "        gamma = params[\"gamma\"],\n",
    "        col_stats     = dataset.col_stats,\n",
    "        col_names_dict= train_dataset.tensor_frame.col_names_dict,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    # 4) epoch-10 학습 + pruning\n",
    "    for epoch in range(1, 11):\n",
    "        train_one_epoch(model, train_loader, optimizer)\n",
    "        val_acc = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        trial.report(val_acc, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_acc                    # maximize\n",
    "\n",
    "\n",
    "# ─────────────────── Optuna Study 실행 ──────────────────── #\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3))\n",
    "study.optimize(objective, n_trials=50, timeout=60*60)      # 예: 50 trial / 1 h\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.value)\n",
    "print(\"Best params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────── 최적 파라미터로 파이널 핏 ──────────────────── #\n",
    "bp = study.best_trial.params\n",
    "train_loader, _, test_loader = make_loaders(bp[\"batch_size\"], use_val=False)  # train+val 통합\n",
    "\n",
    "final_model = TabNet(\n",
    "    out_channels = 2,\n",
    "    num_layers   = bp[\"num_layers\"],\n",
    "    split_attn_channels  = bp[\"channels\"],\n",
    "    split_feat_channels  = bp[\"channels\"],\n",
    "    gamma = bp[\"gamma\"],\n",
    "    col_stats     = dataset.col_stats,\n",
    "    col_names_dict= train_dataset.tensor_frame.col_names_dict,\n",
    ").to(device)\n",
    "\n",
    "optimizer  = torch.optim.Adam(final_model.parameters(), lr=bp[\"lr\"])\n",
    "scheduler  = ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "for epoch in range(1, 11):            # 다시 10 epoch\n",
    "    train_one_epoch(final_model, train_loader, optimizer)\n",
    "    scheduler.step()\n",
    "\n",
    "test_acc = evaluate(final_model, test_loader)\n",
    "print(f\"\\n★ FINAL Accuracy on TEST set: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab_Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "is_classification = True              # 분류 문제\n",
    "\n",
    "# ────────────────── 데이터로더 헬퍼 ────────────────── #\n",
    "def make_loaders(batch_size, use_val=True):\n",
    "    tr_tf, va_tf, te_tf = (train_dataset.tensor_frame,\n",
    "                           val_dataset.tensor_frame,\n",
    "                           test_dataset.tensor_frame)\n",
    "    tr_loader = DataLoader(tr_tf, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va_tf, batch_size=batch_size) if use_val else None\n",
    "    te_loader = DataLoader(te_tf, batch_size=batch_size)\n",
    "    return tr_loader, va_loader, te_loader\n",
    "\n",
    "# ────────────────── 공통 학습·평가 ────────────────── #\n",
    "def train_one_epoch(model, loader, optim):\n",
    "    model.train()\n",
    "    loss_sum = samp = 0\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        loss = F.cross_entropy(model(tf), tf.y.long())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_sum += loss.item() * len(tf.y)\n",
    "        samp += len(tf.y)\n",
    "    return loss_sum / samp\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for tf in loader:\n",
    "        tf = tf.to(device)\n",
    "        pred = model(tf).argmax(dim=-1)\n",
    "        correct += (pred == tf.y).sum().item()\n",
    "        total += len(tf.y)\n",
    "    return correct / total\n",
    "\n",
    "# ────────────────── Optuna objective ────────────────── #\n",
    "def objective(trial):\n",
    "    # 1) 탐색할 파라미터\n",
    "    p = dict(\n",
    "        channels       = trial.suggest_categorical(\"channels\",  [128, 256, 512]),\n",
    "        num_heads      = trial.suggest_categorical(\"num_heads\", [4, 8, 16]),\n",
    "        num_layers     = trial.suggest_int   (\"num_layers\", 2, 6),\n",
    "        encoder_pad_sz = trial.suggest_int   (\"encoder_pad_size\", 1, 4),\n",
    "        attn_dp        = trial.suggest_float (\"attn_dropout\", 0.1, 0.5),\n",
    "        ffn_dp         = trial.suggest_float (\"ffn_dropout\",  0.1, 0.5),\n",
    "        batch_size     = trial.suggest_categorical(\"batch_size\", [128, 256, 512]),\n",
    "        lr             = trial.suggest_float (\"lr\", 1e-5, 1e-3, log=True),\n",
    "    )\n",
    "\n",
    "    # 2) 데이터\n",
    "    tr_loader, va_loader, _ = make_loaders(p[\"batch_size\"], use_val=True)\n",
    "\n",
    "    # 3) 모델\n",
    "    model = TabTransformer(\n",
    "        channels      = p[\"channels\"],\n",
    "        out_channels  = 2,\n",
    "        num_layers    = p[\"num_layers\"],\n",
    "        num_heads     = p[\"num_heads\"],\n",
    "        encoder_pad_size = p[\"encoder_pad_sz\"],\n",
    "        attn_dropout  = p[\"attn_dp\"],\n",
    "        ffn_dropout   = p[\"ffn_dp\"],\n",
    "        col_stats     = dataset.col_stats,\n",
    "        col_names_dict= train_dataset.tensor_frame.col_names_dict,\n",
    "    ).to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=p[\"lr\"])\n",
    "    sched = ExponentialLR(optim, gamma=0.95)\n",
    "\n",
    "    # 4) 10 epoch 학습 + pruning\n",
    "    for ep in range(1, 11):\n",
    "        train_one_epoch(model, tr_loader, optim)\n",
    "        val_acc = evaluate(model, va_loader)\n",
    "        sched.step()\n",
    "\n",
    "        trial.report(val_acc, step=ep)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_acc                     # maximize ACC\n",
    "\n",
    "# ────────────────── Study 실행 ────────────────── #\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=3))\n",
    "study.optimize(objective, n_trials=40, timeout=60*60)   # 40 trial or 1h\n",
    "\n",
    "print(\"◇ Best val ACC:\", study.best_trial.value)\n",
    "print(\"◇ Best params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k:20s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────── 파이널 핏 (train+val) ────────────────── #\n",
    "bp = study.best_trial.params\n",
    "tr_loader, _, te_loader = make_loaders(bp[\"batch_size\"], use_val=False)\n",
    "\n",
    "final_model = TabTransformer(\n",
    "    channels      = bp[\"channels\"],\n",
    "    out_channels  = 2,\n",
    "    num_layers    = bp[\"num_layers\"],\n",
    "    num_heads     = bp[\"num_heads\"],\n",
    "    encoder_pad_size = bp[\"encoder_pad_sz\"],\n",
    "    attn_dropout  = bp[\"attn_dp\"],\n",
    "    ffn_dropout   = bp[\"ffn_dp\"],\n",
    "    col_stats     = dataset.col_stats,\n",
    "    col_names_dict= train_dataset.tensor_frame.col_names_dict,\n",
    ").to(device)\n",
    "\n",
    "optim = torch.optim.Adam(final_model.parameters(), lr=bp[\"lr\"])\n",
    "sched = ExponentialLR(optim, gamma=0.95)\n",
    "\n",
    "for ep in range(1, 11):        # 다시 10 epoch\n",
    "    train_one_epoch(final_model, tr_loader, optim)\n",
    "    sched.step()\n",
    "\n",
    "test_acc = evaluate(final_model, te_loader)\n",
    "print(f\"\\n★ FINAL Test ACC: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
